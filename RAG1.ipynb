{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naman-Ahuja-DS/Rag-finance-chatbot/blob/main/RAG1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "o13Jm3vM_koG",
        "outputId": "a28b5999-0cbb-4f50-ac3c-59ff10c7f0d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.6/330.6 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.5/500.5 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.1/158.1 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-exporter-otlp-proto-common==1.38.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-proto==1.38.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-sdk~=1.38.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mpackage_installed\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit langchain langchain-community langchain-huggingface pypdf sentence-transformers langchain-chroma langchain-core -qU\n",
        "print('package_installed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Okj3zDp_oQZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29220dad-5aed-4cb9-c02c-827d48c1f794"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.documents import Document\n",
        "from langchain_chroma import Chroma\n",
        "import streamlit as st"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DX4dv1rQeCVW",
        "outputId": "b53cca33-5876-46ec-937a-0a27fcd88c86"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-15 17:51:18.201 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-02-15 17:51:18.203 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-02-15 17:51:18.204 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27\n"
          ]
        }
      ],
      "source": [
        "st.write(\"Agrigrader Chatbot\")\n",
        "\n",
        "file_path = '2302.13971v1.pdf'\n",
        "loader = PyPDFLoader(file_path)\n",
        "\n",
        "docs = loader.load()\n",
        "print(len(docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYTEMtdKp5o7",
        "outputId": "9876b7ff-80a5-4f7f-821d-777812ac9037"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLaMA: Open and Efﬁcient Foundation Language Models\n",
            "Hugo Touvron∗, Thibaut Lavril∗, Gautier Izacard∗, Xavier Martinet\n",
            "Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal\n",
            "Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin\n",
            "Edouard Grave∗, Guillaume Lample∗\n",
            "Meta AI\n",
            "Abstract\n",
            "We introduce LLaMA, a collection of founda-\n",
            "tion language models ranging from 7B to 65B\n",
            "parameters. We train our models on trillions\n",
            "of tokens, and show that it is possible to train\n",
            "state-of-the-art models using publicly avail-\n",
            "able datasets exclusively, without resorting\n",
            "to proprietary and inaccessible datasets. In\n",
            "particular, LLaMA-13B outperforms GPT-3\n",
            "(175B) on most benchmarks, and LLaMA-\n",
            "65B is competitive with the best models,\n",
            "Chinchilla-70B and PaLM-540B. We release\n",
            "all our models to the research community1.\n",
            "1 Introduction\n",
            "Large Languages Models (LLMs) trained on mas-\n",
            "sive corpora of texts have shown their ability to per-\n",
            "form new tasks from textual instructions or from a\n",
            "few examples (\n",
            "/n__________________________________________\n",
            "MetaData {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-02-28T01:57:46+00:00', 'author': '', 'keywords': '', 'moddate': '2023-02-28T01:57:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '2302.13971v1.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1'}\n"
          ]
        }
      ],
      "source": [
        "print(docs[0].page_content[:1000])\n",
        "print ('/n__________________________________________')\n",
        "print ('MetaData',docs[0].metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCHyFk-VqJTW",
        "outputId": "99192317-d901-44c9-d30e-b45b4d903e63"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'producer': 'pdfTeX-1.40.21',\n",
              " 'creator': 'LaTeX with hyperref',\n",
              " 'creationdate': '2023-02-28T01:57:46+00:00',\n",
              " 'author': '',\n",
              " 'keywords': '',\n",
              " 'moddate': '2023-02-28T01:57:46+00:00',\n",
              " 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
              " 'subject': '',\n",
              " 'title': '',\n",
              " 'trapped': '/False',\n",
              " 'source': '2302.13971v1.pdf',\n",
              " 'total_pages': 27,\n",
              " 'page': 0,\n",
              " 'page_label': '1'}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[0].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEdzfFswq0MQ",
        "outputId": "c9676135-5c32-4566-c4da-b31a78ef08b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "118"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    add_start_index=True,\n",
        ")\n",
        "\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "len(all_splits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyJ8RAdPu2wc",
        "outputId": "493c191c-6b63-4057-a3a9-44a195ab774d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-02-28T01:57:46+00:00', 'author': '', 'keywords': '', 'moddate': '2023-02-28T01:57:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '2302.13971v1.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'start_index': 832}, page_content='PaLM and LaMDA (Thoppilan et al., 2022). PaLM\\nand LLaMA were trained on datasets that contain\\na similar number of code tokens.\\nAs show in Table 8, for a similar number\\nof parameters, LLaMA outperforms other gen-\\neral models such as LaMDA and PaLM, which\\nare not trained or ﬁnetuned speciﬁcally for code.\\nLLaMA with 13B parameters and more outper-\\nforms LaMDA 137B on both HumanEval and\\nMBPP. LLaMA 65B also outperforms PaLM 62B,\\neven when it is trained longer. The pass@1 results\\nreported in this table were obtained by sampling\\nwith temperature 0.1. The pass@100 and pass@80\\nmetrics were obtained with temperature 0.8. We\\nuse the same method as Chen et al. (2021) to obtain\\nunbiased estimates of the pass@k.\\nIt is possible to improve the performance on code\\nby ﬁnetuning on code-speciﬁc tokens. For instance,\\nPaLM-Coder (Chowdhery et al., 2022) increases\\nthe pass@1 score of PaLM on HumanEval from\\n26.2% for PaLM to 36%. Other models trained\\nspeciﬁcally for code also perform better than gen-')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_splits[26]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QM0PGnX2vC82"
      },
      "outputs": [],
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name='BAAI/bge-small-en-v1.5')\n",
        "\n",
        "vector_store = Chroma( embedding_function=embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TL88Jgb6vtcO"
      },
      "outputs": [],
      "source": [
        "ids = vector_store.add_documents(documents=all_splits)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAZGPPwxxCXA",
        "outputId": "90f01d4e-cbc5-43af-feba-b56fc04fae3d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(118,\n",
              " ['14151a59-5ba6-4727-bb66-86c384558bda',\n",
              "  'eaaf0df3-6316-4832-81ac-b95c41cab34b',\n",
              "  'b8224511-84ea-457c-8676-fe3a83665aff',\n",
              "  'f6587619-a4d1-4ea7-af28-01531b80245d',\n",
              "  'c3628600-8cad-4c6c-9485-1c0c69281976'])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(ids),ids[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGP_G13dxGPo",
        "outputId": "397e2837-4462-4115-f6c8-4ed09bfa5a90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " what is LLaMa\n",
            "page_content='LLaMA: Open and Efﬁcient Foundation Language Models\n",
            "Hugo Touvron∗, Thibaut Lavril∗, Gautier Izacard∗, Xavier Martinet\n",
            "Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal\n",
            "Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin\n",
            "Edouard Grave∗, Guillaume Lample∗\n",
            "Meta AI\n",
            "Abstract\n",
            "We introduce LLaMA, a collection of founda-\n",
            "tion language models ranging from 7B to 65B\n",
            "parameters. We train our models on trillions\n",
            "of tokens, and show that it is possible to train\n",
            "state-of-the-art models using publicly avail-\n",
            "able datasets exclusively, without resorting\n",
            "to proprietary and inaccessible datasets. In\n",
            "particular, LLaMA-13B outperforms GPT-3\n",
            "(175B) on most benchmarks, and LLaMA-\n",
            "65B is competitive with the best models,\n",
            "Chinchilla-70B and PaLM-540B. We release\n",
            "all our models to the research community1.\n",
            "1 Introduction\n",
            "Large Languages Models (LLMs) trained on mas-\n",
            "sive corpora of texts have shown their ability to per-\n",
            "form new tasks from textual instructions or from a' metadata={'subject': '', 'total_pages': 27, 'page': 0, 'creationdate': '2023-02-28T01:57:46+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'trapped': '/False', 'producer': 'pdfTeX-1.40.21', 'source': '2302.13971v1.pdf', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2023-02-28T01:57:46+00:00', 'author': '', 'title': '', 'page_label': '1', 'start_index': 0}\n"
          ]
        }
      ],
      "source": [
        "query = input(' ')\n",
        "results = vector_store.similarity_search(query)\n",
        "print(results[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4lDSIqFxe9C",
        "outputId": "8c5ffeb4-ffa9-49a4-c654-9038f14a1072"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Document(id='14151a59-5ba6-4727-bb66-86c384558bda', metadata={'source': '2302.13971v1.pdf', 'creationdate': '2023-02-28T01:57:46+00:00', 'creator': 'LaTeX with hyperref', 'title': '', 'page': 0, 'subject': '', 'producer': 'pdfTeX-1.40.21', 'keywords': '', 'author': '', 'moddate': '2023-02-28T01:57:46+00:00', 'page_label': '1', 'start_index': 0, 'total_pages': 27, 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'trapped': '/False'}, page_content='LLaMA: Open and Efﬁcient Foundation Language Models\\nHugo Touvron∗, Thibaut Lavril∗, Gautier Izacard∗, Xavier Martinet\\nMarie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal\\nEric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin\\nEdouard Grave∗, Guillaume Lample∗\\nMeta AI\\nAbstract\\nWe introduce LLaMA, a collection of founda-\\ntion language models ranging from 7B to 65B\\nparameters. We train our models on trillions\\nof tokens, and show that it is possible to train\\nstate-of-the-art models using publicly avail-\\nable datasets exclusively, without resorting\\nto proprietary and inaccessible datasets. In\\nparticular, LLaMA-13B outperforms GPT-3\\n(175B) on most benchmarks, and LLaMA-\\n65B is competitive with the best models,\\nChinchilla-70B and PaLM-540B. We release\\nall our models to the research community1.\\n1 Introduction\\nLarge Languages Models (LLMs) trained on mas-\\nsive corpora of texts have shown their ability to per-\\nform new tasks from textual instructions or from a'), 0.6921635866165161)\n"
          ]
        }
      ],
      "source": [
        "embedding = embeddings.embed_query(query)\n",
        "\n",
        "results = vector_store.similarity_search_by_vector_with_relevance_scores(embedding)\n",
        "print(results[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8GEX_XnlyJ4a"
      },
      "outputs": [],
      "source": [
        "retriver = vector_store.as_retriever(search_type='similarity', search_kwargs={'k':3})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BzNCkVsOypfK"
      },
      "outputs": [],
      "source": [
        "import streamlit as st"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "HbEkrLzSe7ue",
        "outputId": "2bd05486-8a19-45ed-909c-75af69f0f321"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-16 03:27:55.246 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-02-16 03:27:55.543 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2026-02-16 03:27:55.544 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-02-16 03:27:55.546 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-02-16 03:27:55.550 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-02-16 03:27:55.556 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-02-16 03:27:55.558 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-02-16 03:27:55.563 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-02-16 03:27:55.564 Session state does not function when running a script without `streamlit run`\n",
            "2026-02-16 03:27:55.569 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-02-16 03:27:55.571 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-02-16 03:27:55.574 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "st.title(\"Agrigrader Chatbot\")\n",
        "st.text_input(\"Write Your Question: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "46aBhjzDfYlf",
        "outputId": "678701b1-152f-4d79-8f27-6bed0d595786"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.173.106.90:8501\u001b[0m\n",
            "\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!streamlit run /usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHb1tiYofchA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}